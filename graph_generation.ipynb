{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/alvaro-montero/9408123808121A36/Ubuntu_Programs/Anaconda/envs/gnn_proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear \n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import networkx as nx\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_books_and_split_to_paragraphs(folder_path):\n",
    "    # Lista para almacenar los párrafos de todos los libros\n",
    "    all_paragraphs = []\n",
    "    \n",
    "    # Recorrer todos los archivos de la carpeta\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Verificar si el archivo tiene una extensión de texto válida\n",
    "        if filename.endswith(\".txt\"):  # Puedes ajustar las extensiones si necesitas\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()\n",
    "                # Dividir el contenido en párrafos usando doble salto de línea\n",
    "                paragraphs = content.split(\"\\n\\n\")\n",
    "                # Limpiar espacios y agregar a la lista general\n",
    "                all_paragraphs.extend([p.strip() for p in paragraphs if p.strip()])\n",
    "    \n",
    "    return all_paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminar_etiquetas(text):\n",
    "    pattern = re.compile(r'<[^>]+>|\\([^)]+\\)|--[^\\-]+--')\n",
    "    doc = pattern.sub('', text)\n",
    "    return doc\n",
    "\n",
    "# Removemos apostrofes para quedarnos con palabras como cant couldnt o dont\n",
    "def remover_apostrofes(text):\n",
    "    pattern = r\"\\\\?'\"\n",
    "    doc = re.sub(pattern, '', text)\n",
    "    return doc\n",
    "# Nos quedamos con solo alfabeto eliminando caracteres especiales    \n",
    "def remover_especiales(text):\n",
    "    pattern = r\"[^a-z.,\\s]\"\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc\n",
    "# Quitamos dobles espacios para tener texto espaciado solamente por un espacio por palabra\n",
    "def dobles_espacios(text):\n",
    "    pattern = r'\\s+'\n",
    "    doc = re.sub(pattern, ' ', text)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_lista_parrafos:  24\n",
      "len_lista_parrafos:  40\n",
      "len_lista_parrafos:  54\n",
      "len_lista_parrafos:  70\n",
      "len_lista_parrafos:  104\n",
      "len_lista_parrafos:  131\n",
      "len_lista_parrafos:  171\n",
      "len_lista_parrafos:  209\n",
      "len_lista_parrafos:  255\n",
      "len_lista_parrafos:  280\n",
      "len_lista_parrafos:  310\n",
      "len_lista_parrafos:  331\n",
      "len_lista_parrafos:  352\n",
      "len_lista_parrafos:  387\n",
      "len_lista_parrafos:  393\n",
      "len_lista_parrafos:  395\n",
      "len_lista_parrafos:  395\n",
      "len_lista_parrafos:  396\n",
      "len_lista_parrafos:  398\n",
      "len_lista_parrafos:  406\n",
      "len_lista_parrafos:  413\n",
      "len_lista_parrafos:  421\n",
      "len_lista_parrafos:  426\n",
      "len_lista_parrafos:  426\n",
      "len_lista_parrafos:  428\n",
      "len_lista_parrafos:  446\n",
      "len_lista_parrafos:  453\n",
      "len_lista_parrafos:  456\n",
      "len_lista_parrafos:  464\n",
      "len_lista_parrafos:  477\n",
      "len_lista_parrafos:  485\n",
      "len_lista_parrafos:  492\n",
      "len_lista_parrafos:  496\n",
      "len_lista_parrafos:  501\n",
      "len_lista_parrafos:  512\n",
      "len_lista_parrafos:  522\n",
      "len_lista_parrafos:  526\n",
      "len_lista_parrafos:  528\n",
      "len_lista_parrafos:  533\n",
      "len_lista_parrafos:  538\n",
      "len_lista_parrafos:  545\n",
      "len_lista_parrafos:  567\n",
      "len_lista_parrafos:  574\n",
      "len_lista_parrafos:  583\n",
      "len_lista_parrafos:  587\n",
      "len_lista_parrafos:  593\n",
      "len_lista_parrafos:  597\n",
      "len_lista_parrafos:  605\n",
      "len_lista_parrafos:  616\n",
      "len_lista_parrafos:  634\n",
      "len_lista_parrafos:  657\n",
      "len_lista_parrafos:  696\n",
      "len_lista_parrafos:  768\n",
      "len_lista_parrafos:  822\n",
      "len_lista_parrafos:  910\n",
      "len_lista_parrafos:  993\n",
      "len_lista_parrafos:  1083\n",
      "len_lista_parrafos:  1149\n",
      "len_lista_parrafos:  1235\n",
      "len_lista_parrafos:  1322\n",
      "len_lista_parrafos:  1397\n",
      "len_lista_parrafos:  1483\n",
      "len_lista_parrafos:  1559\n",
      "1559\n"
     ]
    }
   ],
   "source": [
    "def clean_text_strip(text):\n",
    "   \n",
    "    #Método usando strip() y split()\n",
    "  \n",
    "    lines = text.strip().split('\\r\\n')\n",
    "    return ' '.join(line.strip() for line in lines)\n",
    "\n",
    "def clean_text_regex(text):\n",
    "\n",
    "    # Reemplaza cualquier combinación de \\r y \\n con un espacio\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    # Elimina espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "#texts=load_books_and_split_to_paragraphs(\"/home/alvaro-montero/Escritorio/Repositorios/GNN_Project/Test text\")\n",
    "\n",
    "libros_ids = [1998, 2680, 1232, 5827, 8438, 66048]  # IDs de los libros seleccionados 3623, 2873\n",
    "\n",
    "capitulos = []  # Lista para almacenar capítulos de todos los libros\n",
    "ids_sin_capitulos = []  # IDs de libros sin capítulos\n",
    "\n",
    "lista_parrafos = []\n",
    "\n",
    "# Recorrer cada ID de libro\n",
    "for libro_id in libros_ids:\n",
    "    url = f'https://www.gutenberg.org/cache/epub/{libro_id}/pg{libro_id}-images.html'\n",
    "\n",
    "    # Realizar la solicitud HTTP\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Crear un objeto BeautifulSoup para analizar el HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Encontrar todos los divs con la clase \"chapter\"\n",
    "        chapter_divs = soup.find_all('div', class_='chapter')\n",
    "\n",
    "        if chapter_divs:\n",
    "            # Lista para almacenar todos los capítulos de este libro\n",
    "            libro_capitulos = []\n",
    "\n",
    "            # Recorrer cada capítulo\n",
    "            for chapter in chapter_divs:\n",
    "                # Encontrar todos los párrafos dentro del capítulo\n",
    "                # parrafos = [p.get_text() for p in chapter.find_all('p')]\n",
    "                parrafos = [clean_text_regex(p.get_text()) for p in chapter.find_all('p')]\n",
    "\n",
    "                # Añadir los párrafos del capítulo a la lista de capítulos\n",
    "                if parrafos:  # Solo agregar capítulos con contenido\n",
    "                    libro_capitulos.append(parrafos)\n",
    "\n",
    "                    # Guardar cada parrafo en la lista general de todos los parrafos\n",
    "                    for parrafo in parrafos:\n",
    "                        longitud_parrafo = len(parrafo.split(\" \"))\n",
    "                        if longitud_parrafo > 50: # Umbral para la cantidad de palabras por parrafo \n",
    "                            lista_parrafos.append(parrafo)\n",
    "                            \n",
    "                    print(\"len_lista_parrafos: \", len(lista_parrafos))\n",
    "                    \n",
    "\n",
    "            # Si se encontraron capítulos, almacenarlos con el ID del libro\n",
    "            if libro_capitulos:\n",
    "                capitulos.append((libro_capitulos))\n",
    "        else:\n",
    "            # Si no se encontraron capítulos, agregar el ID a la lista de libros sin capítulos\n",
    "            ids_sin_capitulos.append(libro_id)\n",
    "    else:\n",
    "        print(f\"Error al acceder al libro con ID {libro_id}. Código de estado: {response.status_code}\")\n",
    "\"\"\"\n",
    "texts=load_books_and_split_to_paragraphs(\"/home/alvaro-montero/Escritorio/Repositorios/GNN_Project/Test text\")\n",
    "for text in texts:\n",
    "    text=eliminar_etiquetas(text)\n",
    "    text=remover_apostrofes(text)\n",
    "    text=remover_especiales(text)\n",
    "    text=dobles_espacios(text)\n",
    "\"\"\"\n",
    "\n",
    "#texts=[ parrafo for ]\n",
    "texts=lista_parrafos\n",
    "\n",
    "mat_len=len(texts)\n",
    "#adjw_matrix=torch.zeros(mat_len,mat_len)\n",
    "print(mat_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'But the Unjust man does not always choose actually the greater part, but even sometimes the less; as in the case of things which are simply evil: still, since the less evil is thought to be in a manner a good and the grasping is after good, therefore even in this case he is thought to be a grasping man, i.e. one who strives for more good than fairly falls to his share: of course he is also an unequal man, this being an inclusive and common term.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procesamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "def get_text_embeddings(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():  \n",
    "        outputs = bert_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Usar el token [CLS]\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings de nodos: torch.Size([1559, 768])\n"
     ]
    }
   ],
   "source": [
    "# Dividir textos en lotes\n",
    "batch_size=1500\n",
    "dataloader = torch.utils.data.DataLoader(texts, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "embeddings_list = []\n",
    "with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "            outputs = bert_model(**inputs)\n",
    "            embeddings = outputs.last_hidden_state[:, 0, :]  # Usar el token [CLS]\n",
    "            embeddings_list.append(embeddings)\n",
    "\n",
    "    # Concatenar todos los embeddings en un solo tensor\n",
    "node_embeddings = torch.cat(embeddings_list, dim=0)\n",
    "\n",
    "print(\"Embeddings de nodos:\", node_embeddings.shape)  # Debería ser (num_nodos, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 1.5707e-06, 7.8099e-06,  ..., 5.2077e-21, 2.5639e-30,\n",
      "         1.8543e-21],\n",
      "        [4.2071e-07, 0.0000e+00, 1.7374e-08,  ..., 1.9928e-23, 3.2122e-37,\n",
      "         1.6085e-22],\n",
      "        [1.8212e-06, 1.5126e-08, 0.0000e+00,  ..., 3.4112e-21, 2.3567e-33,\n",
      "         4.3093e-22],\n",
      "        ...,\n",
      "        [2.6521e-11, 3.7887e-13, 7.4494e-11,  ..., 0.0000e+00, 1.4643e-19,\n",
      "         3.4151e-08],\n",
      "        [3.7227e-13, 1.7413e-19, 1.4674e-15,  ..., 4.1750e-12, 0.0000e+00,\n",
      "         2.7601e-12],\n",
      "        [1.2070e-14, 3.9089e-15, 1.2028e-14,  ..., 4.3648e-11, 1.2373e-22,\n",
      "         0.0000e+00]])\n",
      "torch.Size([1559, 1559])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "for i in range(len(node_embeddings)):\n",
    "    for j in range(len(node_embeddings)):\n",
    "        if i==j:\n",
    "            continue\n",
    "        else:\n",
    "            adjw_matrix[i][j] = F.cosine_similarity(node_embeddings[i].unsqueeze(0),node_embeddings[j].unsqueeze(0))\n",
    "\"\"\"\n",
    "\n",
    "scores=torch.matmul(node_embeddings,node_embeddings.T)    \n",
    "\n",
    "mask = torch.eye(scores.size(0), device=scores.device).bool()  # Matriz identidad\n",
    "scores.masked_fill_(mask, float('-inf'))  # Establece -inf en la diagonal\n",
    "adjw_matrix=F.softmax(scores,dim=1)\n",
    "\n",
    "#adjw_matrix=torch.triu(adjw_matrix)\n",
    "print(adjw_matrix)\n",
    "print(adjw_matrix.shape)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(mat_len):\n",
    "    for j in range(mat_len):\n",
    "        if adjw_matrix[i][j]<(1/mat_len):\n",
    "            adjw_matrix[i][j]=0\n",
    "print(adjw_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjw_matrix.cpu()\n",
    "edge_index, edge_weight = torch_geometric.utils.dense_to_sparse(adjw_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ..., 1558, 1558, 1558],\n",
      "        [ 478,  536,  563,  ..., 1228, 1368, 1510]])\n",
      "torch.Size([2, 34344])\n",
      "tensor([0.0216, 0.0037, 0.0202,  ..., 0.0007, 0.0010, 0.0013])\n",
      "torch.Size([34344])\n"
     ]
    }
   ],
   "source": [
    "print(edge_index)\n",
    "print(edge_index.shape)\n",
    "print(edge_weight)\n",
    "print(edge_weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_df=pd.DataFrame(edge_index.numpy())\n",
    "weight_df=pd.DataFrame(edge_weight.numpy())\n",
    "adjw_df=pd.DataFrame(adjw_matrix.numpy())\n",
    "index_df.to_csv(\"/media/alvaro-montero/9408123808121A36/Ubuntu Programs/Project_gnn/index_matrix.csv\",index=False,header=False)\n",
    "weight_df.to_csv(\"/media/alvaro-montero/9408123808121A36/Ubuntu Programs/Project_gnn/weight_matrix.csv\", index=False, header=False)\n",
    "adjw_df.to_csv(\"/media/alvaro-montero/9408123808121A36/Ubuntu Programs/Project_gnn/adjw_matrix.csv\",index=False, header= False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
