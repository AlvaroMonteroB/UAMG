{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear \n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "import networkx as nx\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text_strip(text):\n",
    "   \n",
    "    #Método usando strip() y split()\n",
    "  \n",
    "    lines = text.strip().split('\\r\\n')\n",
    "    return ' '.join(line.strip() for line in lines)\n",
    "\n",
    "def clean_text_regex(text):\n",
    "\n",
    "    # Reemplaza cualquier combinación de \\r y \\n con un espacio\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    # Elimina espacios múltiples\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "#texts=load_books_and_split_to_paragraphs(\"/home/alvaro-montero/Escritorio/Repositorios/GNN_Project/Test text\")\n",
    "\n",
    "libros_ids = [1998, 2680, 1232, 8438, 66048, 3623, 2873, 1004, 1013, 1028, 1059, 1064, 1065,\n",
    "              1080, 1086, 1119, 1399, 1321, 1301, 1260, 1257, 1228, 1227, 1184, 1245, 3010, 3047,\n",
    "              3056, 3059, 3152, 3160, 3207, 3289, 3323, 3332, 3435, 3436, 3437, 3438, 3439, 3440,\n",
    "              3441, 3442, 3443, 3444, 66041, 2500, 2554, 2591, 2600, 2638]  # IDs de los libros seleccionados 3623, 2873, 1998\n",
    "\n",
    "capitulos = []  # Lista para almacenar capítulos de todos los libros\n",
    "ids_sin_capitulos = []  # IDs de libros sin capítulos\n",
    "\n",
    "lista_parrafos = []\n",
    "titulos_libros = []\n",
    "diccionario_subjects = []\n",
    "metadata_grupos_parrafos = []\n",
    "\n",
    "class TimeoutError(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "@timeout_decorator.timeout(5, timeout_exception=TimeoutError)\n",
    "def find_chapters_with_timeout(soup):\n",
    "    \"\"\"\n",
    "    Busca capítulos con un límite de tiempo.\n",
    "    Lanza TimeoutError si excede el tiempo límite.\n",
    "    \"\"\"\n",
    "    return soup.find_all('div', class_='chapter')\n",
    "\n",
    "id_parrafo = 0\n",
    "# Recorrer cada ID de libro\n",
    "for libro_id in libros_ids:\n",
    "    url = f'https://www.gutenberg.org/cache/epub/{libro_id}/pg{libro_id}-images.html'\n",
    "\n",
    "    # Realizar la solicitud HTTP\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        # Crear un objeto BeautifulSoup para analizar el HTML\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        try:\n",
    "            start_time = time.perf_counter()\n",
    "            # Encontrar todos los párrafos dentro del capítulo\n",
    "            parrafos = [clean_text_regex(p.get_text()) for p in soup.find_all('p')]\n",
    "            end_time = time.perf_counter()\n",
    "            search_time = end_time - start_time\n",
    "\n",
    "            # print(f\"Búsqueda completada para libro {libro_id} en {search_time:.2f} segundos\")\n",
    "            \n",
    "\n",
    "            if parrafos and len(parrafos) > 5:\n",
    "                # URL base de la API de Project Gutenberg\n",
    "                url = f\"https://gutendex.com/books/{libro_id}\"  # ID del libro Pride and Prejudice\n",
    "\n",
    "                # Realizar la solicitud HTTP\n",
    "                response = requests.get(url)\n",
    "\n",
    "                # Verificar si la respuesta fue exitosa\n",
    "                if response.status_code == 200:\n",
    "                    metadata = response.json()\n",
    "\n",
    "                    # Obtener el titulo del libro\n",
    "                    title = metadata.get('title')\n",
    "                            \n",
    "                    # Acceder a la sección 'subjects'\n",
    "                    subjects = metadata.get('subjects', [])\n",
    "                    diccionario_subjects.append(subjects[0])\n",
    "                \n",
    "                    title_tag = title\n",
    "\n",
    "                    if title_tag:\n",
    "                        titulo = clean_text_regex(title_tag)\n",
    "                        titulos_libros.append((libro_id, titulo))\n",
    "                        print(f\"id: {libro_id}, Titulo: {titulo}, tiempo : {search_time:.2f} segundos\")\n",
    "                    else:\n",
    "                        titulos_libros.append((libro_id, f\"Título no encontrado para libro {libro_id}\"))\n",
    "                    \n",
    "                    # Lista para almacenar todos los capítulos de este libro\n",
    "                    libro_capitulos = []\n",
    "\n",
    "                    # Limite inferior para el grupo de parrafos\n",
    "                    id_parrafo_inf = id_parrafo\n",
    "\n",
    "                    # libro_capitulos.append(parrafos)\n",
    "                    # Guardar cada parrafo en la lista general de todos los parrafos\n",
    "                    for parrafo in parrafos:\n",
    "                        longitud_parrafo = len(parrafo.split(\" \"))\n",
    "                        if longitud_parrafo > 50: # Umbral para la cantidad de palabras por parrafo \n",
    "                            lista_parrafos.append(parrafo)\n",
    "                            id_parrafo += 1\n",
    "                    # Límite superior para el grupo de parrafos\n",
    "                    id_parrafo_sup = id_parrafo\n",
    "\n",
    "                    # Guardamos la información de la lista de parrafos para el libro actual\n",
    "                    paragraph_data = {\n",
    "                        \"id_libro\": libro_id,\n",
    "                        \"id_nodo_inferior\": id_parrafo_inf,\n",
    "                        \"id_nodo_superior\": id_parrafo_sup,\n",
    "                        \"title\": title,\n",
    "                        \"subject\": subjects[0]\n",
    "                    }\n",
    "\n",
    "                    metadata_grupos_parrafos.append(paragraph_data)\n",
    "\n",
    "                    id_parrafo += 1\n",
    "                                    \n",
    "                    # print(\"len_lista_parrafos: \", len(lista_parrafos))\n",
    "                            \n",
    "\n",
    "                    # Si se encontraron capítulos, almacenarlos con el ID del libro\n",
    "                    if libro_capitulos:\n",
    "                        capitulos.append((libro_capitulos))\n",
    "                else:\n",
    "                    print(\"Error al obtener la metadata. Código de estado:\", response.status_code)\n",
    "            else: \n",
    "                # Si no se encontraron parrafos, agregar el ID a la lista de libros sin parrafos\n",
    "                ids_sin_capitulos.append(libro_id)\n",
    "        except TimeoutError:\n",
    "            print(f\"Timeout al buscar capítulos en libro {libro_id}\")\n",
    "            # Opcional: agregar el libro a una lista de libros que necesitan procesamiento especial\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar libro {libro_id}: {str(e)}\")\n",
    "            continue\n",
    "    else:\n",
    "        print(f\"Error al acceder al libro con ID {libro_id}. Código de estado: {response.status_code}\")\n",
    "\"\"\"\n",
    "texts=load_books_and_split_to_paragraphs(\"/home/alvaro-montero/Escritorio/Repositorios/GNN_Project/Test text\")\n",
    "for text in texts:\n",
    "    text=eliminar_etiquetas(text)\n",
    "    text=remover_apostrofes(text)\n",
    "    text=remover_especiales(text)\n",
    "    text=dobles_espacios(text)\n",
    "\"\"\"\n",
    "\n",
    "#texts=[ parrafo for ]\n",
    "texts=lista_parrafos\n",
    "\n",
    "mat_len=len(texts)\n",
    "#adjw_matrix=torch.zeros(mat_len,mat_len)\n",
    "print(mat_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = bert_model.to(device)  # Move model to GPU\n",
    "def get_text_embeddings(texts):\n",
    "    \"\"\"Get embeddings for a single batch of texts\"\"\"\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Using [CLS] token\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=96\n",
    "dataloader = torch.utils.data.DataLoader(texts, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "embeddings_list = []\n",
    "with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Procesando lotes\", unit=\"lote\"):\n",
    "            batch_embeddings = get_text_embeddings(batch)\n",
    "            embeddings_list.append(batch_embeddings.cpu())\n",
    "\n",
    "    # Concatenar todos los embeddings en un solo tensor\n",
    "node_embeddings = torch.cat(embeddings_list, dim=0)\n",
    "\n",
    "print(\"Embeddings de nodos:\", node_embeddings.shape)  # Debería ser (num_nodos, 768)\n",
    "torch.cuda.synchronize() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "scores=torch.matmul(node_embeddings,node_embeddings.T)    \n",
    "\n",
    "mask = torch.eye(scores.size(0), device=scores.device).bool()  # Matriz identidad\n",
    "scores.masked_fill_(mask, float('-inf'))  # Establece -inf en la diagonal\n",
    "adjw_matrix=F.softmax(scores,dim=1)\n",
    "\n",
    "#adjw_matrix=torch.triu(adjw_matrix)\n",
    "print(adjw_matrix)\n",
    "print(adjw_matrix.shape)\n",
    "threshold = 1/mat_len\n",
    "adjw_matrix = torch.where(adjw_matrix < threshold, \n",
    "                            torch.zeros_like(adjw_matrix), \n",
    "                            adjw_matrix)\n",
    "    \n",
    "torch.cuda.synchronize() \n",
    "print(adjw_matrix)\n",
    "edge_index, edge_weight = torch_geometric.utils.dense_to_sparse(adjw_matrix)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load index and weight\n",
    "index_mat=pd.read_csv(\"/media/alvaro-montero/9408123808121A36/Ubuntu Programs/Project_gnn/index_matrix.csv\",header=None)\n",
    "weight_mat=pd.read_csv(\"/media/alvaro-montero/9408123808121A36/Ubuntu Programs/Project_gnn/weight_matrix.csv\",header=None)\n",
    "\n",
    "index_mat=index_mat.to_numpy()\n",
    "weight_mat=weight_mat.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_graph_data(node_embeddings, adj_matrix, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split a full graph into train and test sets\n",
    "    \n",
    "    Parameters:\n",
    "    - node_embeddings: tensor of shape [num_nodes, embedding_dim]\n",
    "    - adj_matrix: adjacency matrix [num_nodes, num_nodes]\n",
    "    - test_size: proportion of edges to use for testing\n",
    "    \"\"\"\n",
    "    # Get edges from adjacency matrix\n",
    "    edges = adj_matrix.nonzero()\n",
    "    edge_index = torch.tensor(np.array(edges), dtype=torch.long)\n",
    "    \n",
    "    # Get number of nodes\n",
    "    num_nodes = adj_matrix.shape[0]\n",
    "    \n",
    "    # Split edges into train and test\n",
    "    # Convert edge_index to list of tuples for splitting\n",
    "    edge_list = list(zip(edge_index[0].numpy(), edge_index[1].numpy()))\n",
    "    train_edges, test_edges = train_test_split(edge_list, \n",
    "                                             test_size=test_size, \n",
    "                                             random_state=random_state)\n",
    "    \n",
    "    # Convert back to tensors\n",
    "    train_edge_index = torch.tensor(train_edges, dtype=torch.long).t()\n",
    "    test_edge_index = torch.tensor(test_edges, dtype=torch.long).t()\n",
    "    \n",
    "    # Create train adjacency matrix\n",
    "    train_adj = torch.zeros_like(adj_matrix)\n",
    "    train_adj[train_edge_index[0], train_edge_index[1]] = 1\n",
    "    \n",
    "    # Generate negative edges for testing\n",
    "    test_neg_edge_index = negative_sampling(\n",
    "        edge_index=edge_index,\n",
    "        num_nodes=num_nodes,\n",
    "        num_neg_samples=test_edge_index.size(1),\n",
    "        force_undirected=True\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'node_embeddings': node_embeddings,\n",
    "        'train_edge_index': train_edge_index,\n",
    "        'test_pos_edge_index': test_edge_index,\n",
    "        'test_neg_edge_index': test_neg_edge_index,\n",
    "        'train_adj': train_adj,\n",
    "        'full_adj': adj_matrix\n",
    "    }\n",
    "\n",
    "def create_train_test_masks(edge_index, test_edges, num_nodes):\n",
    "    \"\"\"\n",
    "    Create boolean masks for train and test edges\n",
    "    \"\"\"\n",
    "    # Initialize masks\n",
    "    train_mask = torch.ones(edge_index.size(1), dtype=torch.bool)\n",
    "    test_mask = torch.zeros(edge_index.size(1), dtype=torch.bool)\n",
    "    \n",
    "    # Convert test edges to set for faster lookup\n",
    "    test_edges_set = set(map(tuple, test_edges.t().tolist()))\n",
    "    \n",
    "    # Create masks\n",
    "    for idx, (i, j) in enumerate(edge_index.t().tolist()):\n",
    "        if (i, j) in test_edges_set or (j, i) in test_edges_set:\n",
    "            train_mask[idx] = False\n",
    "            test_mask[idx] = True\n",
    "            \n",
    "    return train_mask, test_mask\n",
    "\n",
    "def sample_new_negative_edges(edge_index, num_nodes, num_samples):\n",
    "    \"\"\"\n",
    "    Sample negative edges that don't exist in the graph\n",
    "    \"\"\"\n",
    "    existing_edges = set(map(tuple, edge_index.t().tolist()))\n",
    "    neg_edges = []\n",
    "    \n",
    "    while len(neg_edges) < num_samples:\n",
    "        i = np.random.randint(0, num_nodes)\n",
    "        j = np.random.randint(0, num_nodes)\n",
    "        if i != j and (i, j) not in existing_edges and (j, i) not in existing_edges:\n",
    "            neg_edges.append((i, j))\n",
    "            existing_edges.add((i, j))\n",
    "    \n",
    "    return torch.tensor(neg_edges, dtype=torch.long).t()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/alvaro-montero/9408123808121A36/Ubuntu_Programs/Anaconda/envs/gnn_proj/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch.nn import Linear, ModuleList, Sequential, ReLU, Dropout, Sigmoid\n",
    "\n",
    "class GATLinkLabelPredictor(torch.nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels,     # BERT embedding size (768)\n",
    "                 hidden_channels=256,\n",
    "                 label_dim=1,     # Dimension of node labels\n",
    "                 num_heads=8,\n",
    "                 num_layers=2,\n",
    "                 dropout=0.2):\n",
    "        super(GATLinkLabelPredictor, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # GAT layers for node embedding processing\n",
    "        self.conv_first = GATConv(\n",
    "            in_channels + label_dim,  # Concatenate BERT embeddings with labels \n",
    "            hidden_channels,\n",
    "            heads=num_heads,\n",
    "            dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.conv_hidden = ModuleList([\n",
    "            GATConv(\n",
    "                hidden_channels * num_heads, \n",
    "                hidden_channels,\n",
    "                heads=num_heads,\n",
    "                dropout=dropout\n",
    "            ) for _ in range(num_layers - 1)\n",
    "        ])\n",
    "\n",
    "        # Edge predictor\n",
    "        self.edge_predictor = Sequential(\n",
    "            Linear(hidden_channels * num_heads * 2, hidden_channels),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(hidden_channels, 1),\n",
    "            Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Label predictor for new nodes\n",
    "        self.label_predictor = Sequential(\n",
    "            Linear(hidden_channels * num_heads, hidden_channels),\n",
    "            ReLU(),\n",
    "            Dropout(dropout),\n",
    "            Linear(hidden_channels, label_dim),\n",
    "            Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, labels, edge_index):\n",
    "        # Concatenate node features with labels\n",
    "        x = torch.cat([x, labels], dim=-1)\n",
    "        \n",
    "        # GAT layers\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv_first(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "\n",
    "        for conv in self.conv_hidden:\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.elu(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def predict_links(self, node_embeddings, potential_edges):\n",
    "        \"\"\"Predicts probability of links for given node pairs\"\"\"\n",
    "        edge_features_1 = node_embeddings[potential_edges[0]]\n",
    "        edge_features_2 = node_embeddings[potential_edges[1]]\n",
    "        edge_features = torch.cat([edge_features_1, edge_features_2], dim=1)\n",
    "        return self.edge_predictor(edge_features)\n",
    "\n",
    "    def predict_label(self, node_embeddings):\n",
    "        \"\"\"Predicts labels for nodes based on their embeddings\"\"\"\n",
    "        return self.label_predictor(node_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GATLinkLabelPredictor(\n",
    "    in_channels=768,    # BERT embedding size\n",
    "    hidden_channels=256,\n",
    "    label_dim=3,  # dimension of your labels\n",
    "    num_heads=8\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parámetros entrenables: 7360004\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Parámetros entrenables: {trainable_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_proj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
